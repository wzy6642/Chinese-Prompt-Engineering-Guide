虽然大型语言模型能够根据零样本提示做出回答，但是当面对更加复杂的问题时，使用零样本提示往往难以得到正确的答案。少样本提示可以帮助模型打开上下文学习的功能，在提示中提供示例以引导模型做出更加准确的回复。当然并不是所有语言模型都能够根据少样本提示做出准确的回答，研究表明，只有模型参量达到一定规模时，模型才会具备少样本提示学习的能力。

让我们通过一个例子来演示少样本提示。在这个例子中，任务是根据一个新的单词的词意进行造句。

```latex
Prompt: "张冠李戴" 的意思是把姓张的帽子戴到姓李的头上；把这一方涉及的过程安插给那一方，比喻认错了对象，弄错了事实。
	一个使用 "张冠李戴" 这个词的例句是：写人物传记最怕考证不实，一不小心就会闹出张冠李戴的笑话来。
	"车水马龙"意思是车如流水，马如游龙一般。形容热闹繁华的景象。
	一个使用 "车水马龙" 这个词的例句是：
Output: 去市中心逛街，人流车流非常多，车水马龙，热闹非凡。
```

我们可以观察到，仅仅提供一个例子（即1-shot）就能让模型学会如何完成任务。对于更难的任务，我们可以尝试增加示例的数量（例如3-shot、5-shot、10-shot等）来进行实验。

基于已有的研究，我们在构造少样本提示时可以参考以下几个小技巧：

1. 为每个例子添加一个标签有助于得到较好的模型输出，当你不知道为例子添加什么标签时，随意给其一个词语作为标签都比不给标签强
2. 从标签的真实分布中随机选择标签（而不是均匀分布）有助于提高模型的预测效果

首先，我们来尝试一个随机标签的例子（即将“负面”和“正面”的标签随机分配给样例）。

```latex
Prompt: 这太棒了！// 表示负面情绪
	这很糟糕！// 表示积极情绪
	哇，那部电影真是太棒了！// 表示积极情绪
	多么可怕的节目！// 
Output: 表示负面情绪
```

不难发现即使标签被随机化，我们仍然能得到正确的答案。需要注意的是我们也保留了格式的统一，这也有所帮助。事实上，通过进一步的实验，我们发现ChatGPT模型也能有效处理不同格式的输入样例。举个例子：

```latex
Prompt: 积极情绪，这太棒了！
	这很糟糕！// 表示积极情绪
	积极，哇，那部电影真是太棒了！
	多么可怕的节目！
Output: 第四句：多么可怕的节目啊！（表示负面情绪）
```

上述格式没有一致性，但模型仍然预测出了正确的标签。
