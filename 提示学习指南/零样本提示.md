现今的大语言模型通过在大量数据上进行训练，并经过仔细的参数调整，可以进行零样本学习 Zero-Shot Prompting。我们在前一章节尝试了一些零样本学习的例子，以下是我们使用过的一个例子：

```latex
Prompt: 把这段文字归类为中性、消极或积极。
		文字内容：我认为食物还可以。
		情感分类：
Output: 中性。
```
 
请注意，上面的提示中我们没有为模型提供任何示例 - 这就是零样本能力在发挥作用。

研究表明，[指令微调](https://arxiv.org/abs/2109.01652)能够提高模型的零样本学习效果。指令微调的核心思想是通过在一系列由指令描述的任务上微调预训练语言模型，大幅提高了语言模型在未见过任务上的zero-shot prompting性能。实验结果表明，该方法在60多个自然语言处理任务上均取得了显著的改进，并展示了微调后的模型能够准确地对未见过的样本及任务进行准确的推理。此外，RLHF（从人类反馈中进行强化学习）算法可以使模型的预测结果更加符合人类的偏好。该技术推动了诸如ChatGPT这样模型的发展。我们将在接下来的章节中讨论这些方法。当零样本学习方法无法奏效时，我们建议在提示中提供一些例子，以进行少样本提示。在下一部分中，我们将展示少样本提示的例子。
